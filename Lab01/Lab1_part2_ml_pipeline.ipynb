{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pSvYPtmBj4r"
   },
   "source": [
    "# Lab assignment №1, part 2\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stMK6eNrBj4w"
   },
   "source": [
    "__*This is the second part of the assignment. First and third parts are waiting for you in the same directory.*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7W3-OJ0Bj4x"
   },
   "source": [
    "## Part 2. Data preprocessing, model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1667905584036,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "zvQxm01SzijM"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_pPSMX5Bj4x"
   },
   "source": [
    "### 1. Reading the data\n",
    "Today we work with the [dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29), describing different cars for multiclass ($k=4$) classification problem. The data is available below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1667905584731,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "kIMp-WHQBj4y",
    "outputId": "d97b0e9a-4588-4741-ed39-df3275dffae9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "# If on colab, uncomment the following lines\n",
    "\n",
    "! wget https://raw.githubusercontent.com/girafe-ai/ml-course/22f_basic/homeworks/lab01_ml_pipeline/car_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1667905585729,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "Z7QLZcCcBj4z",
    "outputId": "6dfbcf9a-fee6-4dfc-c275-4f397b669199"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'car_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-62e8fecfba73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'car_data.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxim\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxim\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxim\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxim\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxim\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1218\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1219\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxim\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'car_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('car_data.csv', delimiter=',', header=None).values\n",
    "data = dataset[:, :-1].astype(int)\n",
    "target = dataset[:, -1]\n",
    "\n",
    "print(data.shape, target.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.35)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph1exovTBj40"
   },
   "source": [
    "To get some insights about the dataset, `pandas` might be used. The `train` part is transformed to `pd.DataFrame` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1667905585730,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "DaIU9BrwBj40",
    "outputId": "3b9b45a6-166b-41fd-f76f-a36ecc06b14f"
   },
   "outputs": [],
   "source": [
    "X_train_pd = pd.DataFrame(X_train)\n",
    "\n",
    "# First 15 rows of our dataset.\n",
    "X_train_pd.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXpBzEgzBj41"
   },
   "source": [
    "Methods `describe` and `info` deliver some useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1667905586390,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "UooQ-e7CBj41",
    "outputId": "dc448868-af1e-429d-9d50-fa717e15fcf9"
   },
   "outputs": [],
   "source": [
    "X_train_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1667905586390,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "XZpQKtrNBj42",
    "outputId": "c02cecd7-0642-4e8d-87e6-d200fe599fe3"
   },
   "outputs": [],
   "source": [
    "X_train_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-_ud1dsBj42"
   },
   "source": [
    "### 2. Machine Learning pipeline\n",
    "Here you are supposed to perform the desired transformations. Please, explain your results briefly after each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fJwq9DOBj43"
   },
   "source": [
    "#### 2.0. Data preprocessing\n",
    "* Make some transformations of the dataset (if necessary). Briefly explain the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1dzXVHzRK8I"
   },
   "source": [
    "Firstly, the first feature is index. So let's delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667905586391,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "D68dVW-LRGPt"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "X_train = np.delete(X_train, 0, 1)\n",
    "X_test = np.delete(X_test, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lCVQHL0rpog"
   },
   "source": [
    "It's worth trying to normalize the data, even though the variation of scales is not so large between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667905586392,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "q0BEYTL3rXFC"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1667905586707,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "dWIzHkiBBj43"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939
    },
    "executionInfo": {
     "elapsed": 5128,
     "status": "ok",
     "timestamp": 1667905591834,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "S63hfi3RZxt0",
    "outputId": "b9ec99f8-5a15-485a-b945-9be43226d535"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "sns.heatmap(pd.DataFrame(X_train).corr().abs(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pN5YsB_Kzy5j"
   },
   "source": [
    "Okay, 7-12 are correlated (look at the heatmap). I've also tried to get rid of some strongly correlated features (I made made some plots, etc.), but this did not improve much.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1667905591835,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "svdkC0mUhdrb"
   },
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train, 11, 1)\n",
    "X_test = np.delete(X_test, 11, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1667905591835,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "Ax-6j4L9h9Qj",
    "outputId": "072fd146-0cf3-4f82-f627-ec409b721488"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dq6pwTpBj43"
   },
   "source": [
    "#### 2.1. Basic logistic regression\n",
    "* Find optimal hyperparameters for logistic regression with cross-validation on the `train` data (small grid/random search is enough, no need to find the *best* parameters).\n",
    "\n",
    "* Estimate the model quality with `f1` and `accuracy` scores.\n",
    "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
    "\n",
    "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` `tol=1e-3` and ` max_iter=500`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94293,
     "status": "ok",
     "timestamp": 1667905686099,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "uP6035XKBj44",
    "outputId": "49ea0bc0-874f-4b95-9042-1f024bcb123c"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Define model\n",
    "model = LogisticRegression(tol=1e-3, solver='saga', max_iter=500, multi_class='multinomial')\n",
    "\n",
    "# Define evaluation\n",
    "cv = RepeatedStratifiedKFold(random_state=1)\n",
    "\n",
    "# Define search space; C: inverse of reg strength\n",
    "space = dict()\n",
    "space['penalty'] = ['l1', 'l2', 'elasticnet']\n",
    "space['C'] = list(np.logspace(-5, 5, 10))\n",
    "\n",
    "# Define search\n",
    "search = GridSearchCV(estimator=model, param_grid=space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "\n",
    "# Execute search\n",
    "logCV = search.fit(X_train, y_train)\n",
    "\n",
    "# Results of search\n",
    "print(f\"Best Accuracy: {logCV.best_score_}\")\n",
    "print(f\"Best Params: {logCV.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1667905686529,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "cQMmU7wdWC5X",
    "outputId": "4f63de76-a388-4017-d985-76fa4f330aa5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "# Train our model and predict labels\n",
    "model = LogisticRegression(tol=1e-3, solver='saga', max_iter=500, multi_class='multinomial', C=logCV.best_params_['C'], penalty=logCV.best_params_['penalty'])\n",
    "clf = model.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, None]\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f\"F1-macro: {f1_score(y_test, y_pred, average='macro')}\")\n",
    "print(f'F1: {f1_score(y_test, y_pred, average=None)}')\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f82xexvLr60"
   },
   "source": [
    "Classes 1 and 2 were poorly classified (among themselves in particular). I think I know the reason: we classify a bus, a van and two \"almost the same\" passenger cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5235,
     "status": "ok",
     "timestamp": 1667905691755,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "e00SMV_npIF2",
    "outputId": "eb6256d4-9467-4cb5-be9e-85f96b7e122d"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1667905691755,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "atv6p8Vdx4e8",
    "outputId": "442dcabb-a147-4b18-a26d-a210a21b4ff4"
   },
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_roc\n",
    "\n",
    "probas = clf.predict_proba(X_test)\n",
    "plot_roc(y_test, probas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvyt-NJFBj44"
   },
   "source": [
    "#### 2.2. PCA: explained variance plot\n",
    "* Apply the PCA to the train part of the data. Build the explaided variance plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1667905692159,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "te18_6h5Bj45",
    "outputId": "a0b2a710-4632-4534-9ec0-c2e518ea2a87"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Data is already normalized\n",
    "pca = PCA()\n",
    "pca.fit_transform(X_train)\n",
    "\n",
    "expl_var = pca.explained_variance_\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(range(1, len(expl_var) + 1), expl_var)\n",
    "ax.bar(range(1, len(expl_var) + 1), expl_var, alpha=0.5, color='grey')\n",
    "ax.set_xticks(range(1, 19))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oVvG5rNBj45"
   },
   "source": [
    "#### 2.3. PCA trasformation\n",
    "* Select the appropriate number of components. Briefly explain your choice. Should you normalize the data?\n",
    "\n",
    "*Use `fit` and `transform` methods to transform the `train` and `test` parts.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C57Nma3JMfav"
   },
   "source": [
    "The features after 10 give a small contribution in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667905692160,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "wGsZEb7vBj45"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "# Data is (are) already normalized\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMTHle84Bj46"
   },
   "source": [
    "**Note: From this point `sklearn` [Pipeline](https://scikit-learn.org/stable/modules/compose.html) might be useful to perform transformations on the data. Refer to the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for more information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpHFZDoyBj46"
   },
   "source": [
    "#### 2.4. Logistic regression on PCA-preprocessed data.\n",
    "* Find optimal hyperparameters for logistic regression with cross-validation on the transformed by PCA `train` data.\n",
    "\n",
    "* Estimate the model quality with `f1` and `accuracy` scores.\n",
    "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
    "\n",
    "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` and `tol=1e-3`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667905692160,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "KK24DteSBj46"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43005,
     "status": "ok",
     "timestamp": 1667905735157,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "rFWH7NtB63Af"
   },
   "outputs": [],
   "source": [
    "# Find optimal hyperparams\n",
    "model = LogisticRegression(tol=1e-3, solver='saga', max_iter=500, multi_class='multinomial')\n",
    "\n",
    "cv = RepeatedStratifiedKFold(random_state=1)\n",
    "\n",
    "space = dict()\n",
    "space['penalty'] = ['l1', 'l2', 'elasticnet']\n",
    "space['C'] = list(np.logspace(-5, 5, 10))\n",
    "\n",
    "search = GridSearchCV(estimator=model, param_grid=space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "logCV_pca = search.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 701,
     "status": "ok",
     "timestamp": 1667905735818,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "w7kmiDqT7g75",
    "outputId": "8bb1af36-7c68-46e0-af22-28530b0825c6"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = LogisticRegression(tol=1e-3, solver='saga', max_iter=500, multi_class='multinomial', C=logCV_pca.best_params_['C'], penalty=logCV_pca.best_params_['penalty'])\n",
    "clf = model.fit(X_train_pca, y_train)\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, None]\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f\"F1-macro: {f1_score(y_test, y_pred, average='macro')}\")\n",
    "print(f'F1: {f1_score(y_test, y_pred, average=None)}')\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1667905735820,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "juwzb2Ah8Gnv",
    "outputId": "51660dc6-5eff-46b8-d780-ab41d3325e98"
   },
   "outputs": [],
   "source": [
    "# Plot ROC-Curve\n",
    "probas = clf.predict_proba(X_test_pca)\n",
    "plot_roc(y_test, probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSfVEnHjBj47"
   },
   "source": [
    "#### 2.5. Decision tree\n",
    "* Now train a desicion tree on the same data. Find optimal tree depth (`max_depth`) using cross-validation.\n",
    "\n",
    "* Measure the model quality using the same metrics you used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2056,
     "status": "ok",
     "timestamp": 1667905738365,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "bn8AuCD0Bj47",
    "outputId": "d3b09231-c83a-4687-9bf2-4f7954d14f56"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Find optimal depth\n",
    "model = DecisionTreeClassifier()\n",
    "cv = RepeatedStratifiedKFold(random_state=1)\n",
    "space = {'max_depth': range(3, 15)}\n",
    "search = GridSearchCV(estimator=model, param_grid=space, n_jobs=-1, cv=cv)\n",
    "treeCV = search.fit(X_train, y_train)\n",
    "\n",
    "print(treeCV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1667905738778,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "YqqjoAuRLaal",
    "outputId": "ea7e8552-c60d-41cf-85bc-76362119fd35"
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=treeCV.best_params_['max_depth'])\n",
    "clf = model.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, None]\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f\"F1-macro: {f1_score(y_test, y_pred, average='macro')}\")\n",
    "print(f'F1: {f1_score(y_test, y_pred, average=None)}')\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1667905739259,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "nYiG7W74R0zs",
    "outputId": "68f04b4a-645c-4dc2-ace2-278b5a4464cb"
   },
   "outputs": [],
   "source": [
    "probas = clf.predict_proba(X_test)\n",
    "plot_roc(y_test, probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VHO4WrXBj47"
   },
   "source": [
    "#### 2.6. Bagging.\n",
    "Here starts the ensembling part.\n",
    "\n",
    "First we will use the __Bagging__ approach. Build an ensemble of $N$ algorithms varying N from $N_{min}=2$ to $N_{max}=100$ (with step 5).\n",
    "\n",
    "We will build two ensembles: of logistic regressions and of decision trees.\n",
    "\n",
    "*Comment: each ensemble should be constructed from models of the same family, so logistic regressions should not be mixed up with decision trees.*\n",
    "\n",
    "\n",
    "*Hint 1: To build a __Bagging__ ensebmle varying the ensemble size efficiently you might generate $N_{max}$ subsets of `train` data (of the same size as the original dataset) using bootstrap procedure once. Then you train a new instance of logistic regression/decision tree with optimal hyperparameters you estimated before on each subset (so you train it from scratch). Finally, to get an ensemble of $N$ models you average the $N$ out of $N_{max}$ models predictions.*\n",
    "\n",
    "*Hint 2: sklearn might help you with this taks. Some appropriate function/class might be out there.*\n",
    "\n",
    "* Plot `f1` and `accuracy` scores plots w.r.t. the size of the ensemble.\n",
    "\n",
    "* Briefly analyse the plot. What is the optimal number of algorithms? Explain your answer.\n",
    "\n",
    "* How do you think, are the hyperparameters for the decision trees you found in 2.5 optimal for trees used in ensemble? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3726,
     "status": "ok",
     "timestamp": 1667906162991,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "WCSfwsE2tFf8",
    "outputId": "a2736ac7-75e5-4375-af51-ef9176c89601"
   },
   "outputs": [],
   "source": [
    "# For convinience\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "z6DjbR1TBj47"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PySNC_K8FLIu"
   },
   "source": [
    "**Decision Trees Ensemble:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BdXOUlC8Bj47",
    "outputId": "54ba7ac8-8543-4863-88a5-08dc1af9414c"
   },
   "outputs": [],
   "source": [
    "# Trying to find the best hyperparameters over the subsets\n",
    "tree_bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100)\n",
    "cv = RepeatedStratifiedKFold(random_state=1)\n",
    "space = {'base_estimator__max_depth': range(3, 15)}\n",
    "search = GridSearchCV(estimator=tree_bag, param_grid=space, n_jobs=-1, cv=cv)\n",
    "tree_bag_CV = search.fit(X_train, y_train) \n",
    "# Check the params\n",
    "print(tree_bag_CV.best_params_)\n",
    "# Making optimal base estimator using founded params\n",
    "opt_base=DecisionTreeClassifier(max_depth=tree_bag_CV.best_params_['base_estimator__max_depth']) \n",
    "\n",
    "# Training the model and testing it for size of emsembles \n",
    "f1_macro, f1, accuracy = np.zeros(20), np.zeros((20, 4)), np.zeros(20) \n",
    "N = np.arange(2, 100, 5)\n",
    "for _ in tqdm(range(20)):\n",
    "  for i, n in enumerate(N):  \n",
    "    tree_bag = BaggingClassifier(base_estimator=opt_base, n_estimators=n)\n",
    "    tree_bag.fit(X_train, y_train)\n",
    "    f1_macro[i] = f1_score(y_test, tree_bag.predict(X_test), average='macro')\n",
    "    f1[i] = f1_score(y_test, tree_bag.predict(X_test), average=None)\n",
    "    accuracy[i] = accuracy_score(y_test, tree_bag.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "s7XY7cynY2rV",
    "outputId": "23aefdcb-03ab-4382-bda2-388e3c128546"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(28, 8)) \n",
    "ax1.plot(N, f1_macro)\n",
    "ax1.title.set_text('f1_macro')\n",
    "ax1.set_xticks(N)\n",
    "ax1.grid()\n",
    "ax2.plot(N, f1)\n",
    "ax2.legend(['bus', 'opel', 'saab', 'van'])\n",
    "ax2.title.set_text('f1_classes')\n",
    "ax3.plot(N, accuracy)\n",
    "ax3.title.set_text('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-WqaPWa2XsM"
   },
   "source": [
    "Using the bagging we decrease the variance. We need to have a low bias using the ensemble of trees, so the depth of the trees must be enough. But each tree in the ensemble is trained on bootstrapped samples based on the original data. The data is (are) still slightly different. In this case we've got (almost) the same depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bU3Dcidw7xO9"
   },
   "source": [
    "The optimal number of algorithms is 47."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC5k6zQpaeuN"
   },
   "source": [
    "**Logistic Regression Ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ybT4prZCad5E",
    "outputId": "68f38948-04f7-44c1-eef2-55b827927d54"
   },
   "outputs": [],
   "source": [
    "log_bag = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=100)\n",
    "cv = RepeatedStratifiedKFold(random_state=1)\n",
    "space = {'base_estimator__C': np.logspace(-5, 5, 10)}\n",
    "search = GridSearchCV(estimator=log_bag, param_grid=space, n_jobs=-1, cv=cv)\n",
    "log_bag_CV = search.fit(X_train, y_train) \n",
    "# Check the params\n",
    "print(log_bag_CV.best_params_)\n",
    "# Making optimal base estimator using founded params\n",
    "opt_base=LogisticRegression(C=log_bag_CV.best_params_['base_estimator__C'])\n",
    "\n",
    "f1_macro, f1, accuracy = np.zeros(20), np.zeros((20, 4)), np.zeros(20) \n",
    "N = np.arange(2, 100, 5)\n",
    "for i, n in enumerate(N):\n",
    "  log_bag = BaggingClassifier(base_estimator=opt_base, n_estimators=n)\n",
    "  log_bag.fit(X_train, y_train)\n",
    "  f1_macro[i] = f1_score(y_test, log_bag.predict(X_test), average='macro')\n",
    "  f1[i] = f1_score(y_test, log_bag.predict(X_test), average=None)\n",
    "  accuracy[i] = accuracy_score(y_test, log_bag.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Rn5AS68VasIN",
    "outputId": "f31fcf05-143b-4a3b-f381-f76b879fb4b6"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(28, 8)) \n",
    "ax1.plot(N, f1_macro)\n",
    "ax1.title.set_text('f1_macro')\n",
    "ax1.set_xticks(N)\n",
    "ax1.grid()\n",
    "ax2.plot(N, f1)\n",
    "ax2.legend(['bus', 'opel', 'saab', 'van'])\n",
    "ax2.title.set_text('f1_classes')\n",
    "ax3.plot(N, accuracy)\n",
    "ax3.title.set_text('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKl1YO3_74dw"
   },
   "source": [
    "The optimal number of algorithms is 52."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hoUqrLcBj47"
   },
   "source": [
    "#### 2.7. Random Forest\n",
    "Now we will work with the Random Forest (its `sklearn` implementation).\n",
    "\n",
    "* * Plot `f1` and `accuracy` scores plots w.r.t. the number of trees in Random Forest.\n",
    "\n",
    "* What is the optimal number of trees you've got? Is it different from the optimal number of logistic regressions/decision trees in 2.6? Explain the results briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317297,
     "status": "ok",
     "timestamp": 1667906494415,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "LG3cqncvBj48",
    "outputId": "0ccbbf71-c91b-41b8-c2c7-8a843032671e"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# YOUR CODE HERE\n",
    "rf = RandomForestClassifier(n_estimators=100, max_features='sqrt')\n",
    "cv = RepeatedStratifiedKFold(random_state=1)\n",
    "space = {'max_depth': range(2, 15),\n",
    "         'criterion': ['gini', 'entropy', 'log_loss']}\n",
    "search = GridSearchCV(estimator=rf, param_grid=space, n_jobs=-1, cv=cv)\n",
    "rf_CV = search.fit(X_train, y_train) \n",
    "\n",
    "# Training the model and testing it for size of emsembles \n",
    "f1_macro, f1, accuracy = np.zeros(20), np.zeros((20, 4)), np.zeros(20) \n",
    "N = np.arange(2, 100, 5)\n",
    "for _ in tqdm(range(20)):\n",
    "  for i, n in enumerate(N):  \n",
    "    rf = RandomForestClassifier(n_estimators=n,\n",
    "                                criterion=rf_CV.best_params_['criterion'],\n",
    "                                max_depth=rf_CV.best_params_['max_depth'],\n",
    "                                max_features='sqrt')\n",
    "    rf.fit(X_train, y_train)\n",
    "    f1_macro[i] = f1_score(y_test, rf.predict(X_test), average='macro')\n",
    "    f1[i] = f1_score(y_test, rf.predict(X_test), average=None)\n",
    "    accuracy[i] = accuracy_score(y_test, rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1667906902133,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "gzeoiQry_3pX",
    "outputId": "2122d1ef-aedb-4ec5-ea31-f1491228d687"
   },
   "outputs": [],
   "source": [
    "print(rf_CV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "executionInfo": {
     "elapsed": 1228,
     "status": "ok",
     "timestamp": 1667906499675,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "t_aKAJhtydoc",
    "outputId": "3f653610-7c05-47b9-9ed8-f77c3e347c29"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(28, 8)) \n",
    "ax1.plot(N, f1_macro)\n",
    "ax1.title.set_text('f1_macro')\n",
    "ax1.set_xticks(N)\n",
    "ax1.grid()\n",
    "ax2.plot(N, f1)\n",
    "ax2.legend(['bus', 'opel', 'saab', 'van'])\n",
    "ax2.title.set_text('f1_classes')\n",
    "ax3.plot(N, accuracy)\n",
    "ax3.title.set_text('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWG_Bci9-rbl"
   },
   "source": [
    "The optimal number of algorithms is different from it was on bagging. Random forest uses random features on each node that makes the algorithms less correlated ${\\Rightarrow}$ variance decreases. Using random forest we actually obtained the best metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLhfKcdbBj48"
   },
   "source": [
    "#### 2.8. Learning curve\n",
    "Your goal is to estimate, how does the model behaviour change with the increase of the `train` dataset size.\n",
    "\n",
    "* Split the training data into 10 equal (almost) parts. Then train the models from above (Logistic regression, Desicion Tree, Random Forest) with optimal hyperparameters you have selected on 1 part, 2 parts (combined, so the train size in increased by 2 times), 3 parts and so on.\n",
    "\n",
    "* Build a plot of `accuracy` and `f1` scores on `test` part, varying the `train` dataset size (so the axes will be score - dataset size.\n",
    "\n",
    "* Analyse the final plot. Can you make any conlusions using it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "executionInfo": {
     "elapsed": 45354,
     "status": "ok",
     "timestamp": 1667908014758,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "huNFAaoRBj48",
    "outputId": "cf42fa6a-ed5b-4527-f662-bbae4dca3fab"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "f1_macro, f1, accuracy = np.zeros(9), np.zeros((9, 4)), np.zeros(9)\n",
    "train_size = np.zeros(9)\n",
    "splits = np.linspace(0, .99, 10)\n",
    "for i, split in enumerate(splits[1:]):\n",
    "  # Splitting\n",
    "  X_train_split, _, y_train_split, _ = train_test_split(X_train, y_train, train_size=split)\n",
    "  # Best hyperparams:\n",
    "  estim = LogisticRegression(tol=1e-3, solver='saga', max_iter=500, multi_class='multinomial')\n",
    "  space = {'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "           'C': np.logspace(-5, 5, 10)}\n",
    "  search = GridSearchCV(estimator=estim, param_grid=space, scoring='accuracy', n_jobs=-1, refit=True)\n",
    "  CV = search.fit(X_train_split, y_train_split) \n",
    "  # Fitting the model with the best hyperparams\n",
    "  model = CV.best_estimator_\n",
    "  model.fit(X_train_split, y_train_split)\n",
    "\n",
    "  f1_macro[i] = f1_score(y_test, model.predict(X_test), average='macro')\n",
    "  f1[i] = f1_score(y_test, model.predict(X_test), average=None)\n",
    "  accuracy[i] = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "  train_size[i] = split\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(28, 8)) \n",
    "ax1.plot(train_size, f1_macro)\n",
    "ax1.title.set_text('f1_macro')\n",
    "ax1.set_xticks(train_size)\n",
    "ax1.grid()\n",
    "ax2.plot(train_size, f1)\n",
    "ax2.legend(['bus', 'opel', 'saab', 'van'])\n",
    "ax2.title.set_text('f1_classes')\n",
    "ax3.plot(train_size, accuracy)\n",
    "ax3.title.set_text('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "executionInfo": {
     "elapsed": 4790,
     "status": "ok",
     "timestamp": 1667908033797,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "K4LCelx0nIIA",
    "outputId": "26950af7-4dbe-4b69-944b-7d55654f49bf"
   },
   "outputs": [],
   "source": [
    "f1_macro, f1, accuracy = np.zeros(9), np.zeros((9, 4)), np.zeros(9)\n",
    "train_size = np.zeros(9)\n",
    "splits = np.linspace(0, .99, 10)\n",
    "for i, split in enumerate(splits[1:]):\n",
    "  # Splitting\n",
    "  X_train_split, _, y_train_split, _ = train_test_split(X_train, y_train, train_size=split)\n",
    "  # Best hyperparams:\n",
    "  estim = DecisionTreeClassifier()\n",
    "  space = {'max_depth': range(3, 15), 'criterion': ['gini', 'entropy']}\n",
    "  search = GridSearchCV(estimator=estim, param_grid=space, scoring='accuracy', n_jobs=-1, refit=True)\n",
    "  CV = search.fit(X_train_split, y_train_split) \n",
    "  # Fitting the model with the best hyperparams\n",
    "  model = CV.best_estimator_\n",
    "  model.fit(X_train_split, y_train_split)\n",
    "\n",
    "  f1_macro[i] = f1_score(y_test, model.predict(X_test), average='macro')\n",
    "  f1[i] = f1_score(y_test, model.predict(X_test), average=None)\n",
    "  accuracy[i] = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "  train_size[i] = split\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(28, 8)) \n",
    "ax1.plot(train_size, f1_macro)\n",
    "ax1.title.set_text('f1_macro')\n",
    "ax1.set_xticks(train_size)\n",
    "ax1.grid()\n",
    "ax2.plot(train_size, f1)\n",
    "ax2.legend(['bus', 'opel', 'saab', 'van'])\n",
    "ax2.title.set_text('f1_classes')\n",
    "ax3.plot(train_size, accuracy)\n",
    "ax3.title.set_text('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "executionInfo": {
     "elapsed": 27983,
     "status": "ok",
     "timestamp": 1667908061767,
     "user": {
      "displayName": "Maxim Yanovich",
      "userId": "06383102460373176228"
     },
     "user_tz": -180
    },
    "id": "iwyisOLQBM7N",
    "outputId": "16d7c841-d436-4d43-85de-9550c16cfb2f"
   },
   "outputs": [],
   "source": [
    "f1_macro, f1, accuracy = np.zeros(9), np.zeros((9, 4)), np.zeros(9)\n",
    "train_size = np.zeros(9)\n",
    "splits = np.linspace(0, .99, 10)\n",
    "for i, split in enumerate(splits[1:]):\n",
    "  # Splitting\n",
    "  X_train_split, _, y_train_split, _ = train_test_split(X_train, y_train, train_size=split)\n",
    "  # Best hyperparams:\n",
    "  estim = RandomForestClassifier(n_estimators=32, criterion='gini')\n",
    "  space = {'max_depth': range(3, 15)}\n",
    "  search = GridSearchCV(estimator=estim, param_grid=space, scoring='accuracy', n_jobs=-1, refit=True)\n",
    "  CV = search.fit(X_train_split, y_train_split) \n",
    "  # Fitting the model with the best hyperparams\n",
    "  model = CV.best_estimator_\n",
    "  model.fit(X_train_split, y_train_split)\n",
    "\n",
    "  f1_macro[i] = f1_score(y_test, model.predict(X_test), average='macro')\n",
    "  f1[i] = f1_score(y_test, model.predict(X_test), average=None)\n",
    "  accuracy[i] = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "  train_size[i] = split\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(28, 8)) \n",
    "ax1.plot(train_size, f1_macro)\n",
    "ax1.title.set_text('f1_macro')\n",
    "ax1.set_xticks(train_size)\n",
    "ax1.grid()\n",
    "ax2.plot(train_size, f1)\n",
    "ax2.legend(['bus', 'opel', 'saab', 'van'])\n",
    "ax2.title.set_text('f1_classes')\n",
    "ax3.plot(train_size, accuracy)\n",
    "ax3.title.set_text('accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [
    "-dq6pwTpBj43",
    "fvyt-NJFBj44",
    "4oVvG5rNBj45",
    "xpHFZDoyBj46"
   ],
   "provenance": [
    {
     "file_id": "https://github.com/girafe-ai/ml-course/blob/22f_basic/homeworks/lab01_ml_pipeline/Lab1_part2_ml_pipeline.ipynb",
     "timestamp": 1666283061729
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
